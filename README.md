# Automated Mouse Behavior Recognition
This project addresses the recognition of mouse behaviors in videos by using CNN embeddings and LSTM or TCN networks.


This repository shows the code implemented to support my Master Thesis. The report is shown in the pdf file: [TFM_XAVI_DE_JUAN](TFM_XAVI_DE_JUAN.pdf)

## Project organization

The main structure of the project is the following:

    .
    ├── config
    ├── data
    ├── notebooks
    ├── results
    ├── src
    ├── AUTHORS.md
    ├── LICENSE
    ├── README.md
    └── TFM_XAVI_DE_JUAN.pdf

Each folder has a conseptual explanation:

- **config**: Configuration files used throughout the project.
- **data**: Path where all data should be stored.
- **notebooks**: Notebooks that show the pipelines in a Jupyter notebook style.
- **results**: Path where results from different pipelines are stored.
- **src**: Project source code in python.

Deeping into these folders there is:

### config
    .
    ├── requirements.txt
    └── settings.yaml

- **requirements.txt**: File where environment dependencies are stored.
- **settings.yaml**: Specific project parameters.

### data
    .
    ├── logs
    ├── processed
    └── raw

- **logs**: Where the file logs are stored.
- **processed**: Where any kind of intermediate file is stored.
- **raw**: Where main input files should be placed.

### results
    .
    ├── evaluation
    └── hp_search

- **evaluation**: Where results from the evaluation pipeline are stored.
- **hp_search**: Where results from the hyperparameter search pipeline are stored.

### src
    .
    ├── data
    ├── models
    ├── tools
    ├── dataset.py
    ├── evaluate.py
    └── hp_search.py

- **data**: Contains the code that read/process/write data.
- **models**: Contains the code related to modelling.
- **tools**: Contains the generic code used throughout the project.
- **dataset.py**: Contains the main file to run dataset creation pipeline.
- **evaluate.py**: Contains the main file to run the evaluation pipeline.
- **hp_search.py**: Contains the main file to run the hyperparameter search pipeline.

## Usage

In order to use the code provided in this project there are some steps that must be followed before running the notebooks or the python scripts containing each pipeline.

### Requirements
- Python 3+
- virtualenv

### Setup

1. Create a virtual environment:
```
virtualenv venv
```

2. Activate the environment
```
source venv/bin/activate
```

3. Install the dependencies
```
pip install -r config/requirements.txt
```

### Run
On the one side, the `notebooks` folder contains some notebooks that can be run by just starting a jupyter notebook session, selecting the virtual environment created as kernel and running each cell. Note that the notebook are numbered, that means that should be run in that order due to dependencies between them.

On the other hand, the source folder contains 3 main files that can run three different pipelines:
1. `dataset.py`: Creates the dataset by using the raw videos stored in the `data/raw` folder.
2. `evaluation.py`: Runs a training and an evaluation based on the dataset created by the dataset pipeline which should be stored in the `data/processed` folder. The pipeline stores the results in the `results/evaluation` folder.
3. `hp_search.py`: Runs a hyperparameter search pipeline based on the dataset generated by the first pipeline, stored in the `data/processed` folder and stores the results in the `results/hp_search` folder.

Then, in order to run the previous pipelines, the command line commands are the following (each script must be run from the root folder):

1. Dataset creation pipeline:
```
python -m src.dataset
```

2. Hyperparameter search pipeline:
```
python -m src.hp_search <model>
```
`<model>` is an argument from the script where the user must specify which model to use. Available models are: [`resnet.LSTM`, `inception_resnet.LSTM`, `resnet.TCN`, `inception_resnet.TCN`]

3. Evaluation pipeline:
```
python -m src.evaluation <model>
```
In the same way, `<model>` is an argument with the same available options as the previous pipeline: [`resnet.LSTM`, `inception_resnet.LSTM`, `resnet.TCN`, `inception_resnet.TCN`]

**Note**: one could run these pipelines without making any change, but which hyperparameters are tuned in the second pipeline? which hyperparameters are used in the evaluation pipeline?

To answer these questions, I invite you to take a look at the `config/settings.yaml` file where you can see different configuration parameters. Among them, there are the `hyperparameter_search` and the `evalution` parameters which specify among others the hyperparameters used (or tuned) in each pipeline.